{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Large Language Model - Model\n",
    "\n",
    "In this workshop, you will learn how to fine tune the prompts and the LLMs to enhance and improves its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, GenerationConfig, TrainingArguments\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (12460, 4), 'validation': (500, 4), 'test': (1500, 4)}\n",
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load and explore the following datasets\n",
    "\n",
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# print the shape\n",
    "print(dataset.shape)\n",
    "\n",
    "# print dataset keys\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = id\n",
      "v = train_50\n",
      "k = dialogue\n",
      "v = #Person1#: You have the right to remain silent. Anything you say can and will be used against you in a court of law. You have the right to have an attorney present during questioning. If you cannot afford an attorney, one will be appointed for you. Do you understand?\n",
      "#Person2#: Yes.\n",
      "#Person1#: What's your name?\n",
      "#Person2#: My name is James.\n",
      "#Person1#: What's your nationality?\n",
      "#Person2#: American.\n",
      "#Person1#: What's your relationship with the victim?\n",
      "#Person2#: I don't know him.\n",
      "#Person1#: Why did you attack the victim?\n",
      "#Person2#: Because he beat me first when I tried to stop him from grabbing my bag and running away.\n",
      "#Person1#: How many times did you stab the victim?\n",
      "#Person2#: I stabbed his belly three times.\n",
      "#Person1#: Did you know that your actions might cause serous injuries or death?\n",
      "#Person2#: I knew, but I couldn't control myself.\n",
      "#Person1#: Was it your intention to kill the victim?\n",
      "#Person2#: No. I didn't kill him on purpose, madam. It's him who caused the incident. I need to see my attorney.\n",
      "#Person1#: OK. Give me his number and we'll contact him.\n",
      "k = summary\n",
      "v = #Person1# stabbed the victim because he beat #Person1# first and tried to grab #Person1#'s bag. #Person1# says he didn't kill him on purpose.\n",
      "k = topic\n",
      "v = interrogate the murderer\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print a record\n",
    "idx = 50\n",
    "for k, v in dataset['train'][idx].items():\n",
    "   print(f'k = {k}\\nv = {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the LLM model\n",
    "\n",
    "In this workshop we will be turning the <code>google/flan-t5-base</code> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to dump a model's tunable parameters\n",
    "def print_trainable_model_params(model):\n",
    "   trainable_model_params = 0\n",
    "   all_model_params = 0\n",
    "   for _, param in model.named_parameters():\n",
    "      all_model_params += param.numel()\n",
    "      if param.requires_grad:\n",
    "         trainable_model_params += param.numel()\n",
    "   return f\"Trainable parameters: {trainable_model_params}\\nTotal parameters: {all_model_params}\\nPercentable of trainable parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 247577856\n",
      "Total parameters: 247577856\n",
      "Percentable of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of trainable parameters\n",
    "print(print_trainable_model_params(base_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dialogue dataset\n",
    "\n",
    "We will train the model to summarize dialogue by creating a dialogue-summary pair for the LLM to process. The dialogue is the training data and the summary is the label. This is supervized learning.\n",
    "\n",
    "The prompt will be as follows\n",
    "\n",
    "```\n",
    "Summarize the following dialogue.\\n\n",
    "\\n\n",
    "Fred: ...\\n\n",
    "Barney: ...\\n\n",
    "\\n\n",
    "Summary:\\n\n",
    "Summary of the conversation between Fred and Barney\n",
    "```\n",
    "\n",
    "The prompt and the summary will be tokenized for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utitlity function to prepare the data for training \n",
    "# Tokenize function\n",
    "def tokenize_fn(data):\n",
    "   start_prompt = 'Summarize the following dialogue.\\n\\n'\n",
    "   end_prompt = '\\n\\nSummary:'\n",
    "   prompt = [ start_prompt + d + end_prompt for d in data['dialogue'] ]\n",
    "   summary = data['summary']\n",
    "   #data['proc_prompt'] = prompt\n",
    "   data['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "   data['labels'] = tokenizer(summary, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6851d2a666a74937b84bb8b7b08d12dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cd159c6089482f8f3e4a06227139de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ec77d2ba0c4e7f9c554acf77884ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: prepare the data for training with the tokenize_fn function\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = id, v = train_50\n",
      "k = dialogue, v = #Person1#: You have the right to remain silent. Anything you say can and will be used against you in a court of law. You have the right to have an attorney present during questioning. If you cannot afford an attorney, one will be appointed for you. Do you understand?\n",
      "#Person2#: Yes.\n",
      "#Person1#: What's your name?\n",
      "#Person2#: My name is James.\n",
      "#Person1#: What's your nationality?\n",
      "#Person2#: American.\n",
      "#Person1#: What's your relationship with the victim?\n",
      "#Person2#: I don't know him.\n",
      "#Person1#: Why did you attack the victim?\n",
      "#Person2#: Because he beat me first when I tried to stop him from grabbing my bag and running away.\n",
      "#Person1#: How many times did you stab the victim?\n",
      "#Person2#: I stabbed his belly three times.\n",
      "#Person1#: Did you know that your actions might cause serous injuries or death?\n",
      "#Person2#: I knew, but I couldn't control myself.\n",
      "#Person1#: Was it your intention to kill the victim?\n",
      "#Person2#: No. I didn't kill him on purpose, madam. It's him who caused the incident. I need to see my attorney.\n",
      "#Person1#: OK. Give me his number and we'll contact him.\n",
      "k = summary, v = #Person1# stabbed the victim because he beat #Person1# first and tried to grab #Person1#'s bag. #Person1# says he didn't kill him on purpose.\n",
      "k = topic, v = interrogate the murderer\n",
      "k = input_ids, v = [12198, 1635, 1737, 8, 826, 7478, 5, 1713, 345, 13515, 536, 4663, 10, 148, 43, 8, 269, 12, 2367, 11237, 5, 21345, 25, 497, 54, 11, 56, 36, 261, 581, 25, 16, 3, 9, 1614, 13, 973, 5, 148, 43, 8, 269, 12, 43, 46, 4917, 915, 383, 822, 53, 5, 156, 25, 1178, 5293, 46, 4917, 6, 80, 56, 36, 7817, 21, 25, 5, 531, 25, 734, 58, 1713, 345, 13515, 357, 4663, 10, 2163, 5, 1713, 345, 13515, 536, 4663, 10, 363, 31, 7, 39, 564, 58, 1713, 345, 13515, 357, 4663, 10, 499, 564, 19, 2549, 5, 1713, 345, 13515, 536, 4663, 10, 363, 31, 7, 39, 1157, 485, 58, 1713, 345, 13515, 357, 4663, 10, 797, 5, 1713, 345, 13515, 536, 4663, 10, 363, 31, 7, 39, 1675, 28, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 27, 278, 31, 17, 214, 376, 5, 1713, 345, 13515, 536, 4663, 10, 1615, 410, 25, 3211, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 2070, 3, 88, 3853, 140, 166, 116, 27, 1971, 12, 1190, 376, 45, 3, 26910, 82, 2182, 11, 1180, 550, 5, 1713, 345, 13515, 536, 4663, 10, 571, 186, 648, 410, 25, 3, 17001, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 27, 3, 17001, 4143, 112, 17719, 386, 648, 5, 1713, 345, 13515, 536, 4663, 10, 3963, 25, 214, 24, 39, 2874, 429, 1137, 142, 8283, 5157, 42, 1687, 58, 1713, 345, 13515, 357, 4663, 10, 27, 2124, 6, 68, 27, 2654, 31, 17, 610, 1512, 5, 1713, 345, 13515, 536, 4663, 10, 2751, 34, 39, 8762, 12, 5781, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 465, 5, 27, 737, 31, 17, 5781, 376, 30, 1730, 6, 11454, 265, 5, 94, 31, 7, 376, 113, 2953, 8, 5415, 5, 27, 174, 12, 217, 82, 4917, 5, 1713, 345, 13515, 536, 4663, 10, 6902, 5, 6434, 140, 112, 381, 11, 62, 31, 195, 574, 376, 5, 20698, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "k = labels, v = [1713, 345, 13515, 536, 4663, 3, 17001, 4143, 8, 7584, 250, 3, 88, 3853, 1713, 345, 13515, 536, 4663, 166, 11, 1971, 12, 6131, 1713, 345, 13515, 536, 4663, 31, 7, 2182, 5, 1713, 345, 13515, 536, 4663, 845, 3, 88, 737, 31, 17, 5781, 376, 30, 1730, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify prepared data\n",
    "for k, v in tokenized_dataset['train'][idx].items():\n",
    "   print(f'k = {k}, v = {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove id, dialogue, summary and topic columns from dataset. We only want input_ids and labels\n",
    "cleaned_dataset = tokenized_dataset.remove_columns(\n",
    "   ['id', 'dialogue', 'summary', 'topic' ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = input_ids, v = [12198, 1635, 1737, 8, 826, 7478, 5, 1713, 345, 13515, 536, 4663, 10, 148, 43, 8, 269, 12, 2367, 11237, 5, 21345, 25, 497, 54, 11, 56, 36, 261, 581, 25, 16, 3, 9, 1614, 13, 973, 5, 148, 43, 8, 269, 12, 43, 46, 4917, 915, 383, 822, 53, 5, 156, 25, 1178, 5293, 46, 4917, 6, 80, 56, 36, 7817, 21, 25, 5, 531, 25, 734, 58, 1713, 345, 13515, 357, 4663, 10, 2163, 5, 1713, 345, 13515, 536, 4663, 10, 363, 31, 7, 39, 564, 58, 1713, 345, 13515, 357, 4663, 10, 499, 564, 19, 2549, 5, 1713, 345, 13515, 536, 4663, 10, 363, 31, 7, 39, 1157, 485, 58, 1713, 345, 13515, 357, 4663, 10, 797, 5, 1713, 345, 13515, 536, 4663, 10, 363, 31, 7, 39, 1675, 28, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 27, 278, 31, 17, 214, 376, 5, 1713, 345, 13515, 536, 4663, 10, 1615, 410, 25, 3211, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 2070, 3, 88, 3853, 140, 166, 116, 27, 1971, 12, 1190, 376, 45, 3, 26910, 82, 2182, 11, 1180, 550, 5, 1713, 345, 13515, 536, 4663, 10, 571, 186, 648, 410, 25, 3, 17001, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 27, 3, 17001, 4143, 112, 17719, 386, 648, 5, 1713, 345, 13515, 536, 4663, 10, 3963, 25, 214, 24, 39, 2874, 429, 1137, 142, 8283, 5157, 42, 1687, 58, 1713, 345, 13515, 357, 4663, 10, 27, 2124, 6, 68, 27, 2654, 31, 17, 610, 1512, 5, 1713, 345, 13515, 536, 4663, 10, 2751, 34, 39, 8762, 12, 5781, 8, 7584, 58, 1713, 345, 13515, 357, 4663, 10, 465, 5, 27, 737, 31, 17, 5781, 376, 30, 1730, 6, 11454, 265, 5, 94, 31, 7, 376, 113, 2953, 8, 5415, 5, 27, 174, 12, 217, 82, 4917, 5, 1713, 345, 13515, 536, 4663, 10, 6902, 5, 6434, 140, 112, 381, 11, 62, 31, 195, 574, 376, 5, 20698, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "k = labels, v = [1713, 345, 13515, 536, 4663, 3, 17001, 4143, 8, 7584, 250, 3, 88, 3853, 1713, 345, 13515, 536, 4663, 166, 11, 1971, 12, 6131, 1713, 345, 13515, 536, 4663, 31, 7, 2182, 5, 1713, 345, 13515, 536, 4663, 845, 3, 88, 737, 31, 17, 5781, 376, 30, 1730, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify dataset again\n",
    "for k, v in cleaned_dataset['train'][idx].items():\n",
    "   print(f'k = {k}, v = {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model with pre-processed dataset\n",
    "\n",
    "We will use [<code>Trainer</code>](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#api-reference%20][%20transformers.Trainer) to train the original model. The training result will be written out. The trainer will be configure with [<code>TrainingArgument</code>](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:  False\n"
     ]
    }
   ],
   "source": [
    "# CUDA information\n",
    "\n",
    "print('CUDA available: ', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "   print('B16 supported: ', torch.cuda.is_bf16_supported())\n",
    "   torch.cuda.set_device(0)\n",
    "   print('Current device: ', torch.cuda.current_device())\n",
    "   print('CUDA device name: ', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the LLM Model with Low-Rank Adaptation (LoRA) / Parameter Efficient Fine Tuning (PEFT)\n",
    "\n",
    "We will add a LoRA adapter to the LLM (flan-t5-base) and train the adapter. The original LLM will be frozen. The adapter can be combined with the original LLM during inferencing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "   r=32,\n",
    "   lora_alpha = 32,\n",
    "   target_modules = [ 'q', 'v' ],\n",
    "   lora_dropout=0.05, \n",
    "   bias = \"none\",\n",
    "   task_type = TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add LoRA to the LLM model to be trained\n",
    "lora_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 251,116,800 || trainable%: 1.4093\n",
      "\n",
      "Trainable parameters: 247577856\n",
      "Total parameters: 247577856\n",
      "Percentable of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of parameters, compare LoRA to the original model\n",
    "lora_model.print_trainable_parameters()\n",
    "print()\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(print_trainable_model_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train model with LoRA\n",
    "output_dir = \"lora-training-summary\"\n",
    "\n",
    "lora_training_args = TrainingArguments(\n",
    "   output_dir = output_dir,\n",
    "   auto_find_batch_size = True,\n",
    "   learning_rate = 1e-3,\n",
    "   num_train_epochs = 1,\n",
    "   logging_first_step = 1,\n",
    "   max_steps = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create trainer and train model\n",
    "trainer = Trainer(\n",
    "   model = lora_model, \n",
    "   args = lora_training_args,\n",
    "   train_dataset = cleaned_dataset['train'],\n",
    "   eval_dataset = cleaned_dataset['validation']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model \n",
    "trained_model_name = \"my-flan-t5-base\"\n",
    "\n",
    "trainer.model.save_pretrained(trained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a trained LoRA model\n",
    "\n",
    "The training will take a few hours and over many iterations.\n",
    "\n",
    "For the purpose of this workshop we use a save model [intotheverse/peft-dialogue-summary-checkpoint](https://huggingface.co/intotheverse/peft-dialogue-summary-checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440c7b1fdd9e47d593021b3f5b978006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/334 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c04050ff634021853fcb7fee4bca8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 251,116,800 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#TODO: Load the original model and add the pre-trained LoRA adaptation to the model\n",
    "peft_dialogue_summary_checkpoint = 'intotheverse/peft-dialogue-summary-checkpoint'\n",
    "\n",
    "# load the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "   base_model, \n",
    "   peft_dialogue_summary_checkpoint,\n",
    "   torch_dtype=torch.bfloat16,\n",
    "   is_trainable=False\n",
    ")\n",
    "\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: #Person1# tells #Person2# to pull on the strip and peel off\n",
      "Actual: #Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate LoRA model against the original \n",
    "\n",
    "idx = 50\n",
    "dialogue = dataset['test'][idx]['dialogue']\n",
    "summary = dataset['test'][idx]['summary']\n",
    "\n",
    "# start_prompt = 'Summarize the following dialogue.\\n\\n'\n",
    "# end_prompt = '\\n\\nSummary:'\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Summarize the following dialogue.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "#print(tokenized_prompt)\n",
    "\n",
    "result_enc = lora_model.generate(input_ids=tokenized_prompt['input_ids'])\n",
    "\n",
    "result = tokenizer.decode(result_enc[0], skip_special_tokens=True)\n",
    "print(f'Completed: {result}')\n",
    "\n",
    "print(f'Actual: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_model_summary</th>\n",
       "      <th>lora_model_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "\n",
       "                              original_model_summary  \\\n",
       "0  The memo is to be distributed to all employees...   \n",
       "1  The memo is to be distributed to all employees...   \n",
       "2  The memo is to be distributed to all employees...   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "\n",
       "                                  lora_model_summary  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic and #Person1# s...  \n",
       "4  #Person2# got stuck in traffic and #Person1# s...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for evaluation\n",
    "dialogues = []\n",
    "summaries = []\n",
    "orig_model_summaries = []\n",
    "lora_model_summaries = []\n",
    "config = GenerationConfig(max_new_tokens=200)\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "for i in range(5):\n",
    "   print(f'i = {i}')\n",
    "   d = dataset['test'][i]['dialogue']\n",
    "   s = dataset['test'][i]['summary']\n",
    "   prompt = f\"Summarize the following conversation.\\n\\n{d}\\n\\nSummary:\"\n",
    "   tokenized_prompt = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "   orig_resp = original_model.generate(input_ids=tokenized_prompt, generation_config=config)\n",
    "   orig_resp_text = tokenizer.decode(orig_resp[0], skip_special_tokens=True)\n",
    "   lora_resp = lora_model.generate(input_ids=tokenized_prompt, generation_config=config)\n",
    "   lora_resp_text = tokenizer.decode(lora_resp[0], skip_special_tokens=True)\n",
    "\n",
    "   summaries.append(s)\n",
    "   orig_model_summaries.append(orig_resp_text)\n",
    "   lora_model_summaries.append(lora_resp_text)\n",
    "\n",
    "zipped_summaries = list(zip(summaries, orig_model_summaries, lora_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns=['label', 'original_model_summary', 'lora_model_summary'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models with ROUGE/Bleu metrics\n",
    "\n",
    "Recall-Oriented Understudy for Gisting Evaluate ([ROUGE](https://pub.aimind.so/unveiling-the-power-of-rouge-metrics-in-nlp-b6d3f96d3363)) is a set of metrics used to evaluate the quality of machine-generated text, such as summaries and translations. ROUGE metrics compare the generated text to a human-written reference and measure the overlap between the two. \n",
    "\n",
    "The metrics range between 0 and 1, with higher scores indicating higher similarity between the baseline and generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create ROUGE metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the original model's result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate with Bleu metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
