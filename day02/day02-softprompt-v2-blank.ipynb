{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Prompt Tuning\n",
    "\n",
    "Prompt tuning adds a small set of trainable virtual tokens (soft prompts) to the input while keeping the pre-trained model's weight frozen. \n",
    "These virtual prompts are not human-readable; they are appended to the start of a prompt to serve as a task-specific guide during LLM training or inferences.\n",
    "\n",
    "For example, the virtual tokens are prefixed to text for sentiment classification:\n",
    "\n",
    "```\n",
    "[virtual tokens] I love Fridays!\n",
    "```\n",
    "\n",
    "where `[virtual tokens]` are the inserted embeddings. These virtual tokens can be randomly generated or initialized from a vocabulary.\n",
    "\n",
    "During training, these token embeddings are updated while the base model remains frozen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloomz-560m\"\n",
    "\n",
    "username = 'ought/raft'\n",
    "dataset_name = \"twitter_complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1112b42ddb0148f1973c9b56d1a8c9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8157ca2d514f449296918febe5ca62cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "raft.py:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9d4ac33a2e405f9998b6afc648a14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)witter_complaints%2Ftrain%2F0000.parquet:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12ebf00e94d4e1ca57544bac0689ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "twitter_complaints%2Ftest%2F0000.parquet:   0%|          | 0.00/266k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eb3939d1c54bff876c6bb3e575ea21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db77267395964292a9f641eaa5ccfa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Load dataset\n",
    "dataset = load_dataset(username, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test'])\n",
      "{'Tweet text': Value(dtype='string', id=None), 'ID': Value(dtype='int32', id=None), 'Label': ClassLabel(names=['Unlabeled', 'complaint', 'no complaint'], id=None)}\n",
      "ClassLabel(names=['Unlabeled', 'complaint', 'no complaint'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Display dataset \n",
    "print(dataset.keys())\n",
    "\n",
    "print(dataset['train'].features)\n",
    "\n",
    "print(dataset['train'].features['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tweet text': '@EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.', 'ID': 3, 'Label': 1}\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "\n",
    "print(dataset['train'][idx])\n",
    "\n",
    "classes = ['Unlabeled', 'complaint', 'no complaint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c32b4779ce407c9dcdabb0e8cfa692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7251aba36b45e185551f069156af13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Add text_label from Label\n",
    "# Call dataset_enh\n",
    "dataset_enh = dataset.map(\n",
    "   lambda x: { 'text_label': [ classes[label] for label in x['Label']] },\n",
    "   batched = True,\n",
    "   num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tweet text': '@EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.', 'ID': 3, 'Label': 1, 'text_label': 'complaint'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_enh['train'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load tokenizer\n",
    "# call variable name tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_class_length = max([\n",
    "   len(tokenizer(class_label)['input_ids']) for class_label in classes \n",
    "])\n",
    "\n",
    "print(max_class_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the max length of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to preprocess teweets\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    text_column = 'Tweet text'\n",
    "    label_column = 'text_label'\n",
    "    max_length = 64\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6648aa83ce74341bb80214972c1c85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531efad239da47af9df1aec90165ad4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Preprocess the Tweets\n",
    "# Call preprocessed tweets dataset_processed\n",
    "dataset_processed = dataset_enh.map(\n",
    "   preprocess_function,\n",
    "   batched=True,\n",
    "   num_proc=1,\n",
    "   remove_columns=dataset_enh['train'].column_names,\n",
    "   load_from_cache_file = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 227985, 5484, 915, 2566, 15157, 4867, 14731, 165189, 2021, 769, 11528, 7220, 35025, 530, 27937, 149533, 1965, 43435, 163255, 1141, 3611, 17, 30655, 632, 1119, 17, 77658, 915, 210, 16449, 5952, 3], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16449, 5952, 3]}\n",
      "Tweet text : @EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this. Label : complaint\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore preprocessed Tweets\n",
    "print(dataset_processed['train'][idx])\n",
    "\n",
    "decode_text = tokenizer.decode(dataset_processed['train'][idx]['input_ids'], skip_special_tokens=True)\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create train and evaluation dataset \n",
    "# Must call train set DataLoader loader_train\n",
    "# Must call test set DataLoader loader_test\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dataset_train = dataset_processed['train']\n",
    "dataset_test = dataset_processed['test']\n",
    "\n",
    "loader_train = DataLoader(\n",
    "   dataset_train, collate_fn=default_data_collator,\n",
    "   batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "loader_test = DataLoader(\n",
    "   dataset_test, collate_fn=default_data_collator,\n",
    "   batch_size=batch_size, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create PEFT configuration\n",
    "config = PromptTuningConfig(\n",
    "   task_type=TaskType.CAUSAL_LM,\n",
    "   prompt_tuning_init = PromptTuningInit.TEXT,\n",
    "   prompt_tuning_init_text = \"Classify if the tweet is a complain or not:\",\n",
    "   num_virtual_tokens = 8,\n",
    "   tokenizer_name_or_path=model_name\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create model for training\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "softprompt_model = get_peft_config(model, peft_config=config)\n",
    "print(softprompt_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "lr = 3e-2\n",
    "num_epochs = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "   optimizer=optimizer,\n",
    "   num_warmup_steps=0,\n",
    "   num_training_steps=(len(loader_train) * num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# default to CPU\n",
    "device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(loader_train)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(loader_test)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(loader_test)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(loader_train)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = peft_model_id = \"stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42edc5f18bec46758f762003d48664f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigscience/bloomz-560m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3adfe229e04efb8bc5720fab9fbade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   8%|7         | 83.9M/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4f0ce67f70477dad39cc664b17f5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/33.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Load trained model\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "print(config.base_model_name_or_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[227985, 106659,   2566,  71187,  29069,   1387,  55907,   1809,   3784,\n",
      "            578,  17444,    613,   3478,   2592,  13802,     17,   6728,   8256,\n",
      "           1119, 227284,     34,  77658,     29,    210]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tweet text: @husband The pipe has been leaking for over one month. Can fix this NOW? Label: complaint\n"
     ]
    }
   ],
   "source": [
    "## TODO: Write and encode text\n",
    "\n",
    "#input_text = \"Tweet text: @uni I hate the food in your canteen. Label: \" \n",
    "input_text = \"Tweet text: @husband The pipe has been leaking for over one month. Can fix this NOW? Label: \"\n",
    "\n",
    "input_enc = tokenizer(input_text, return_tensors='pt')\n",
    "print(input_enc)\n",
    "\n",
    "output_enc = peft_model.generate(\n",
    "   input_ids = input_enc['input_ids'],\n",
    "   attention_mask = input_enc['attention_mask'],\n",
    "   eos_token_id = 3   \n",
    ")\n",
    "\n",
    "output = tokenizer.decode(output_enc[0], skip_special_tokens=True)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Determine sentiment from model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
