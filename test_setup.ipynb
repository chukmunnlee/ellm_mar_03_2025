{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your setup\n",
    "\n",
    "Run the following simple notebook to test your setup. Each of the cells perform different tasks. Feel free to experiment with the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9990044236183167}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sentiment analysis\n",
    "text = \"I like Saturdays\"\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = sentiment_analysis(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bfd160da23498a808c6e3d34860331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe25c3be4ba49f39d0da06c1761c851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d06a4c62474bdc9b1b9a8818f0d82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb33b3c2a32479d81eeadd102602ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be349905ec344ef4b9241f9680976cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd02361ae25b4dc9abff88000c192b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \" The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner . Scrooge's name was good upon 'Change\"}]\n"
     ]
    }
   ],
   "source": [
    "# Text summary\n",
    "\n",
    "text = \"\"\" \n",
    "Marley was dead, to begin with. There is no doubt whatever about that. The register of his burial was signed by the clergyman, the clerk, the undertaker, \n",
    "and the chief mourner. Scrooge signed it. And Scrooge's name was good upon 'Change for anything he chose to put his hand to. Old Marley was as dead as a door-nail.\n",
    "\"\"\"\n",
    "\n",
    "summarization = pipeline('summarization')\n",
    "result = summarization(text, min_length=5, max_length=40)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 3f49326 (https://huggingface.co/google/vit-base-patch16-224).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 'score': 0.9973016977310181}, {'label': 'white wolf, Arctic wolf, Canis lupus tundrarum', 'score': 0.0006416419055312872}, {'label': 'Arctic fox, white fox, Alopex lagopus', 'score': 0.0005438799853436649}, {'label': 'brown bear, bruin, Ursus arctos', 'score': 0.000208047466003336}, {'label': 'American black bear, black bear, Ursus americanus, Euarctos americanus', 'score': 0.00011899494711542502}]\n"
     ]
    }
   ],
   "source": [
    "# Image classification \n",
    "\n",
    "image = 'https://images.ctfassets.net/i04syw39vv9p/4fCAgc3QtfScIGpQiC9xcY/f54afebdbb96b710f66be360c3d85889/Top-5-Favorite-Mom-and-Cub-Facts-02.jpeg'\n",
    "\n",
    "image_classifier = pipeline('image-classification')\n",
    "\n",
    "result = image_classifier(image)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to ydshieh/vit-gpt2-coco-en and revision 5bebf1e (https://huggingface.co/ydshieh/vit-gpt2-coco-en).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "loading configuration file config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_name_or_path\": \"ydshieh/vit-gpt2-coco-en\",\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 50256,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 50256,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"encoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"ViTModel\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"vit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": true,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_name_or_path\": \"ydshieh/vit-gpt2-coco-en\",\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 50256,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 50256,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"encoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"ViTModel\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"vit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": true,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256\n",
      "}\n",
      "\n",
      "Instantiating ViTModel model under default dtype torch.float32.\n",
      "Attempting to create safetensors variant\n",
      "Instantiating GPT2LMHeadModel model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256\n",
      "}\n",
      "\n",
      "Safetensors PR exists\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing VisionEncoderDecoderModel.\n",
      "\n",
      "All the weights of VisionEncoderDecoderModel were initialized from the model checkpoint at ydshieh/vit-gpt2-coco-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisionEncoderDecoderModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file vocab.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/vocab.json\n",
      "loading file merges.txt from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/merges.txt\n",
      "loading file tokenizer.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file preprocessor_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/preprocessor_config.json\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "Device set to use cuda:0\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'a man with a beard and glasses '}]\n"
     ]
    }
   ],
   "source": [
    "image_to_text = pipeline('image-to-text')\n",
    "\n",
    "result = image_to_text(image)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading configuration file preprocessor_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/preprocessor_config.json\n",
      "loading configuration file preprocessor_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/preprocessor_config.json\n",
      "loading configuration file config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_name_or_path\": \"ydshieh/vit-gpt2-coco-en\",\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 50256,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 50256,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"encoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"ViTModel\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"vit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": true,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/vocab.json\n",
      "loading file merges.txt from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/merges.txt\n",
      "loading file tokenizer.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_name_or_path\": \"ydshieh/vit-gpt2-coco-en\",\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": 50256,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 50256,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"encoder\": {\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"ViTModel\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"vit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": true,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/pytorch_model.bin\n",
      "Instantiating VisionEncoderDecoderModel model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256\n",
      "}\n",
      "\n",
      "Instantiating ViTModel model under default dtype torch.bfloat16.\n",
      "Attempting to create safetensors variant\n",
      "Instantiating GPT2LMHeadModel model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256\n",
      "}\n",
      "\n",
      "Safetensors PR exists\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing VisionEncoderDecoderModel.\n",
      "\n",
      "All the weights of VisionEncoderDecoderModel were initialized from the model checkpoint at ydshieh/vit-gpt2-coco-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisionEncoderDecoderModel for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file vocab.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/vocab.json\n",
      "loading file merges.txt from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/merges.txt\n",
      "loading file tokenizer.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/cmlee/.cache/huggingface/hub/models--ydshieh--vit-gpt2-coco-en/snapshots/5bebf1e9bb163535699a3c53fe47859fa088791c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoImageProcessor, AutoTokenizer\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=660x495 at 0x7E8E16AB9460>\n"
     ]
    }
   ],
   "source": [
    "image = load_image('images/einstein.jpg')\n",
    "image = load_image('images/polar_bears.jpg')\n",
    "\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[0.3333, 0.3490, 0.3569,  ..., 0.3725, 0.3647, 0.3569],\n",
      "          [0.3333, 0.3490, 0.3569,  ..., 0.3725, 0.3725, 0.3647],\n",
      "          [0.3333, 0.3490, 0.3569,  ..., 0.3725, 0.3725, 0.3725],\n",
      "          ...,\n",
      "          [0.6000, 0.6157, 0.6157,  ..., 0.2863, 0.3412, 0.4118],\n",
      "          [0.6235, 0.6314, 0.6471,  ..., 0.2471, 0.2314, 0.2314],\n",
      "          [0.6471, 0.6471, 0.6706,  ..., 0.3725, 0.3490, 0.3333]],\n",
      "\n",
      "         [[0.4745, 0.4745, 0.4667,  ..., 0.4824, 0.4745, 0.4667],\n",
      "          [0.4745, 0.4745, 0.4667,  ..., 0.4824, 0.4824, 0.4745],\n",
      "          [0.4745, 0.4745, 0.4667,  ..., 0.4824, 0.4824, 0.4824],\n",
      "          ...,\n",
      "          [0.6000, 0.6157, 0.6157,  ..., 0.3412, 0.3961, 0.4667],\n",
      "          [0.6235, 0.6314, 0.6392,  ..., 0.3176, 0.2941, 0.2941],\n",
      "          [0.6471, 0.6471, 0.6627,  ..., 0.4118, 0.3882, 0.3804]],\n",
      "\n",
      "         [[0.5529, 0.5529, 0.5529,  ..., 0.5686, 0.5608, 0.5529],\n",
      "          [0.5529, 0.5529, 0.5529,  ..., 0.5686, 0.5686, 0.5608],\n",
      "          [0.5529, 0.5529, 0.5529,  ..., 0.5686, 0.5686, 0.5686],\n",
      "          ...,\n",
      "          [0.6000, 0.6157, 0.6157,  ..., 0.4667, 0.4902, 0.5373],\n",
      "          [0.6235, 0.6314, 0.6392,  ..., 0.4588, 0.4353, 0.4353],\n",
      "          [0.6471, 0.6471, 0.6706,  ..., 0.5373, 0.5294, 0.5137]]]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_info = image_processor(images=[image], return_tensors='pt')\n",
    "\n",
    "print(image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 11545, 13559, 13062,   389,  5586,   319,   257, 46742,  4417,\n",
      "           220, 50256]])\n"
     ]
    }
   ],
   "source": [
    "result = model.generate(pixel_values=image_info.pixel_values)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two polar bears are sitting on a snowy surface \n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
