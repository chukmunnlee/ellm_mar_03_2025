{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sshleifer/distilbart-cnn-12-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveler, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "\n",
    "Then took the other, as just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "\n",
    "And both that morning equally lay\n",
    "In leaves no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and I—\n",
    "I took the one less traveled by,\n",
    "And that has made all the difference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a prompt\n",
    "prompt = f\"\"\" \n",
    "Summarize this article.\n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  1437, 50118, 38182,  3916,  2072,    42,  1566,     4, 50118,\n",
      "          1437, 50118,  9058,  3197, 13105,  4462,    11,    10,  5718,  5627,\n",
      "             6, 50118,  2409,  6661,    38,   115,    45,  1504,   258, 50118,\n",
      "          2409,    28,    65, 33489,     6,   251,    38,  3359, 50118,  2409,\n",
      "          1415,   159,    65,    25,   444,    25,    38,   115, 50118,  3972,\n",
      "           147,    24, 18822,    11,     5,   223, 14596,   131, 50118, 50118,\n",
      "         12948,   362,     5,    97,     6,    25,    95,    25,  2105,     6,\n",
      "         50118,  2409,   519,  2532,     5,   357,  2026,     6, 50118, 10105,\n",
      "            24,    21,  6964,   219,     8,   770,  3568,   131, 50118, 26223,\n",
      "            25,    13,    14,     5,  3133,    89, 50118, 32054, 10610,   106,\n",
      "           269,    59,     5,   276,     6, 50118, 50118,  2409,   258,    14,\n",
      "           662,  6681,  4477, 50118,  1121,  3607,   117,  1149,    56, 14168,\n",
      "         39893,   909,     4, 50118,  7516,     6,    38,  1682,     5,    78,\n",
      "            13,   277,   183,   328, 50118, 34995,  4730,   141,   169,  3315,\n",
      "            15,     7,   169,     6, 50118,   100, 30329,   114,    38,   197,\n",
      "           655,   283,   124,     4, 50118, 50118,   100,  5658,    28,  2758,\n",
      "            42,    19,    10, 27305, 50118,   104, 25741, 10859,  4864,     8,\n",
      "          4864, 12113,    35, 50118,  9058,  3197, 13105,  4462,    11,    10,\n",
      "          5627,     6,     8,    38,   578, 50118,   100,   362,     5,    65,\n",
      "           540,  8468,    30,     6, 50118,  2409,    14,    34,   156,    70,\n",
      "             5,  2249,     4, 50140, 50118, 47977,    35, 50118,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: tokenize the text\n",
    "prompt_enc = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "print(prompt_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     0,  1596,  3197, 13105,  4462,    11,    10,  5627,     6,\n",
      "             8,    38,   578,   100,   578,   362,     5,    65,   540,  8468,\n",
      "             2]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Generate summary with model \n",
    "\n",
    "config = GenerationConfig(min_length=5, max_length=10)\n",
    "\n",
    "answer_enc = model.generate(prompt_enc['input_ids'], min_new_tokens=5, max_new_tokens=20)\n",
    "\n",
    "print(answer_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Two roads diverged in a wood, and I—I— took the one less traveled\n"
     ]
    }
   ],
   "source": [
    "# Decode the answer\n",
    "answer = tokenizer.decode(answer_enc[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Models\n",
    "\n",
    "The <code>flan-t5</code> is a Text-To-Text Transfer Transformer (T5) that is capable of performing zero-shot NLP task such as summary, simple reasoninig, answering questions, etc. \n",
    "\n",
    "Some T5 models from Huggingface\n",
    "- [<code>google/flan-t5-base</code>](https://huggingface.co/google/flan-t5-base)\n",
    "- [<code>google/flan-t5-small</code>](https://huggingface.co/google/flan-t5-small)\n",
    "- [<code>google/flan-t5-xl</code>](https://huggingface.co/google/flan-t5-xl)\n",
    "- [<code>google/flan-t5-xxl</code>](https://huggingface.co/google/flan-t5-xxl) - full model\n",
    "\n",
    "Complete list of [T5 models](https://huggingface.co/models?search=google/flan) on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Perform summarization with google/flan-t5-base model\n",
    "t5_model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_model)\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(t5_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[12198,  1635,  1737,    48,  1108,     5,  2759,  7540, 12355,  5402,\n",
      "            16,     3,     9,  4459,  1679,     6,   275,  8032,    27,   228,\n",
      "            59,  1111,   321,   275,    36,    80,  1111,    49,     6,   307,\n",
      "            27,  8190,   275,  2299,   323,    80,    38,   623,    38,    27,\n",
      "           228,   304,   213,    34, 21222,    16,     8,   365, 24690,   117,\n",
      "            37,    29,   808,     8,   119,     6,    38,   131,    38,  2725,\n",
      "             6,   275,   578,  2361,     8,   394,  1988,     6,  2070,    34,\n",
      "            47,  5956,    63,    11,  1114,  2112,   117,  4229,    38,    21,\n",
      "            24,     8,  5792,   132, 10118,  8842,   135,   310,    81,     8,\n",
      "           337,     6,   275,   321,    24,  1379,  7509,  8260,    86,  3231,\n",
      "           150,  1147,   141, 10968,    26,   537,  1001,     5,  3359,     6,\n",
      "            27,  2697,     8,   166,    21,   430,   239,    55,  5201,  4265,\n",
      "           149,   194,  3433,    30,    12,   194,     6,    27,  3228,    15,\n",
      "            26,     3,    99,    27,   225,   664,   369,   223,     5,    27,\n",
      "          1522,    36,  5188,    48,    28,     3,     9,     3,     7,  9031,\n",
      "           886,  8352,     3,  2568,    11,     3,  2568, 10321,    10,  2759,\n",
      "          7540, 12355,  5402,    16,     3,     9,  1679,     6,    11,    27,\n",
      "           318,    27,   808,     8,    80,   705, 15458,    57,     6,   275,\n",
      "            24,    65,   263,    66,     8,  1750,     5, 20698,    10,     3,\n",
      "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "prompt_t5_enc = tokenizer_t5(prompt, return_tensors='pt')\n",
    "print(prompt_t5_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    37,    29,    27,  8190,     6,    11,  2299,   323,    80,\n",
      "            38,   623,    38,    27,   228,     6,   304,   213,    34, 21222,\n",
      "            16]])\n"
     ]
    }
   ],
   "source": [
    "answer_t5_enc = model_t5.generate(prompt_t5_enc['input_ids'], min_new_tokens=5, max_new_tokens=20)\n",
    "print(answer_t5_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then I stood, and looked down one as far as I could, To where it bent in\n"
     ]
    }
   ],
   "source": [
    "answer_t5 = tokenizer_t5.decode(answer_t5_enc[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer_t5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
